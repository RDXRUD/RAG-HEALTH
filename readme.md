
# ğŸ§  RAG-HEALTH: WHO-focused Retrieval-Augmented Generation System
project link: https://drive.google.com/file/d/1e15xKkPw1KtUhfN2a43LJ2b7T8nXV2RV/view?usp=drive_link

RAG-HEALTH is an end-to-end intelligent question-answering system designed for the healthcare domain. It leverages **Retrieval-Augmented Generation (RAG)** to retrieve information from trusted documents and generate accurate, explainable answers using state-of-the-art LLMs via the **Groq API**.

This system supports multimodal content (text, tables, and images) and is built to enable public access to healthcare knowledge using AI in a safe, transparent, and modular way.

---

## ğŸŒ Key Features

- ğŸ” **Vector-based document retrieval** using ChromaDB and MiniLM embeddings
- ğŸ¤– **LLM-powered answer generation** with Groq's LLaMA3 models
- ğŸ–¼ï¸ **Image-to-text captioning** using ImgBB and Vision LLM
- ğŸ“Š **PDF extraction for tables, text, and images**
- âš™ï¸ **Microservice architecture** using FastAPI
- ğŸ§© **Workflow orchestration** using `n8n`

---

## ğŸ“ Directory Structure

```
RAG-HEALTH/
â”‚
â”œâ”€â”€ data/                         # Source PDF files
â”œâ”€â”€ helpers/                      # Core modules for embedding, retrieval, and LLM
â”‚   â”œâ”€â”€ context_builder.py
â”‚   â”œâ”€â”€ db_init.py
â”‚   â”œâ”€â”€ image_llm.py
â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â”œâ”€â”€ text_llm.py
â”‚
â”œâ”€â”€ scripts/                      # Utility and automation scripts
â”‚   â”œâ”€â”€ data_ingestion.py        # Extracts content and indexes it
â”‚   â”œâ”€â”€ RAG_HEALTH.json          # n8n workflow config
â”‚
â”œâ”€â”€ services/                     # API layer (FastAPI microservices)
â”‚   â”œâ”€â”€ retriever.py             # Context fetcher
â”‚   â”œâ”€â”€ main.py                  # LLM responder
â”‚
â”œâ”€â”€ ui/                           # Streamlit frontend
â”‚   â”œâ”€â”€ chat.py
â”‚
â”œâ”€â”€ vector_store/                # Persisted Chroma vector DB
â”œâ”€â”€ .env                         # API keys and environment variables
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª How It Works

1. **PDF Ingestion**:  
   - `scripts/data_ingestion.py` extracts **text**, **images**, and **tables**.
   - Images are uploaded to ImgBB and captioned using Groq's LLM Vision model.
   - All content is split into semantic chunks and stored in **Chroma** with metadata.

2. **Query Flow**:
   - User enters a healthcare query (e.g., *"What are sterilization guidelines before surgery?"*).
   - The `retriever` service searches Chroma for relevant document chunks.
   - The `main` service sends the prompt to **Groq LLaMA3** API and returns the answer.

3. **Multimodal Understanding**:
   - Tables and images from PDFs are treated as context via captions or markdown rendering.

4. **n8n Workflow**:
   - The entire query â†’ retrieval â†’ response flow is orchestrated using **n8n**.

---

## ğŸ” n8n Workflow (Visual Overview)

```
Trigger Node (Webhook /query)
    â†“
HTTP Node â†’ POST /retrieve
    â†“
HTTP Node â†’ POST /ask
    â†“
Respond to Webhook
```

- Webhook Endpoint: `/query`
- Expected Body:
```json
{
  "query": "Explain laparoscopic sterilization steps."
}
```

- Intermediate Responses:
  - `/retrieve` â†’ `{ context, question }`
  - `/ask` â†’ `{ answer, sources, context }`

---

## ğŸ’» Running Locally

### 1. Clone the Repository

```bash
git clone https://github.com/yourname/RAG-HEALTH
cd RAG-HEALTH
```

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure Environment

Create a `.env` file with the following:

```env
GROQ_API_KEY=your_groq_key
IMGBB_API_KEY=your_imgbb_key
HUGGINGFACEHUB_API_TOKEN=your_huggingface_token
CHROMA_DB_DIR=./vector_store
```

### 4. Ingest Documents

```bash
python scripts/data_ingestion.py
```

### 5. Start Backend Services

```bash
# In one terminal
uvicorn services.retriever:app --port 8000

# In another terminal
uvicorn services.main:app --port 8001
```

### 6. Start n8n

- Import `scripts/RAG_HEALTH.json` into your n8n dashboard
- Start the workflow manually or via API trigger

### 7. (Optional) Launch Streamlit UI

```bash
streamlit run ui/chat.py
```

---

## ğŸ¤– Technologies Used

| Layer         | Technology                         |
|---------------|------------------------------------|
| LLM           | Groq LLaMA3-70B via OpenAI API     |
| Embeddings    | HuggingFace `all-MiniLM-L6-v2`     |
| Vector Store  | Chroma DB                          |
| Backend       | FastAPI                            |
| Frontend      | Streamlit                          |
| Image Upload  | ImgBB                              |
| Orchestration | n8n                                |
| Parsing PDFs  | PyMuPDF (fitz), pdfplumber         |

---

## ğŸ§  Prompt Design

```txt
[Context]
- Chunked and cleaned healthcare-relevant paragraphs
- Captions from PDF images and tables

[Question]
12 Healthy habits?.

[Answer]
(Generated using Groq LLM)
```

---

## ğŸ”® Future Enhancements

- [x] Image + table support
- [x] Text-to-prompt automation
- [ ] Session-level memory
- [ ] Incremental PDF updates
- [ ] Integrated query analytics dashboard
- [ ] Slack / Discord bot interface
- [ ] Speech-to-text input with Whisper

---

## ğŸ§‘â€âš•ï¸ Ideal Use Cases

- WHO guidelines
- Precautions during catastrophe


---

## ğŸ“ƒ License

This project is licensed under the **MIT License**.

---
